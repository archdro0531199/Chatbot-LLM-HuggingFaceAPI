import re
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from datetime import datetime

#url = "https://api-inference.huggingface.co/models/openlm-research/open_llama_7b_v2"
#token = "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"  # Replace with your Hugging Face token

model_name = "openlm-research/open_llama_7b_v2"

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype = torch.float16,
    device_map = "auto"
)

def detect_language(text):
    if re.search(r'[\u4e00-\u9fff]', text):
        return 'zh'
    return 'en'
    
def format_history(history):
    
    conversation = ""
    
    for turn in history:
        if turn["role"] == "user":
            conversation += f"<|start_header_id|user|end_header_id|>\n{turn['text']}<|eot_id|>"
        elif turn["role"] == "bot":
            conversation += f"<start_header_id|>assistant<|end_header_id|>\n{turn['text']}<|eot_id|>"
    return conversation
    
def build_prompt(user_input, history):
    lang = detect_language(user_input)
    history_text = format_history(history)
    
    if lang == 'zh':
        system_prompt = (
            "<|start_header_id|>system<|end_header_id|>\n"
            "你是一位專業的醫療領域智慧助手，熟悉常見疾病、生理機能、健康促進與臨床流程，"
            "請以清楚、簡潔、具臨床專業的方式用中文回答提問。"
            "若你無法確定問題的答案，請回應「我建議您諮詢專業醫師」以避免錯誤資訊。\n"
            "<|eot_id|>"
        )
    else:
        system_prompt = (
            "<|start_header_id|>system<|end_header_id|>\n"
            "You are a professional medical assistant AI. You are familiar with common diseases, physiology, healthcare protocols, "
            "and hospital workflows. Please answer clearly, concisely, and in a clinically accurate manner in English. "
            "If unsure about the answer, say: 'I recommend consulting a healthcare professional.'\n"
            "<|eot_id|>"
        )
    
    
    current_turn = f"<|start_header_id|>user<|end_header_id|>\n{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>"
    prompt = f"<|begin_of_text|>{system_prompt}{history_text}{current_turn}"
    return prompt

def llm(user_input, history = None):
    history = history or []
    prompt = build_prompt(user_input, history)

    inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(model.device)
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    with torch.no_grad():
        print(f"[DEBUG] 開始 generate()：{datetime.now().strftime('%H:%M:%S')}")

        output_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=128,              # 減少輸出長度（避免 GPU 卡）
            max_time=10,                     # 最多 10 秒就結束
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )

        print(f"[DEBUG] 結束 generate()：{datetime.now().strftime('%H:%M:%S')}")

    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return output.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
    



